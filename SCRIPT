# -*- coding: utf-8 -*-
"""Assesment2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y5iMS049P-gZKWVQSscvIGmwULHu9tU6
"""

# Loading up Libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import math
import scipy.stats as stats
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.preprocessing import OneHotEncoder
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import StackingClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_validate
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
from sklearn.model_selection import GridSearchCV

# Loading Dataset
df = pd.read_csv('student dropout.csv')

# Displaying first few rows
print(df.head(50))

# Checking for missing values
print(df.isnull().sum())

# dropping variables we dont need
non_beneficial_columns = [
    'Nacionality',
    'Educational special needs',
    'Inflation rate',
    'Mother\'s qualification',
    'Curricular units 1st sem (without evaluations)',
    'Marital status',
    'Previous qualification',
    'Curricular units 2nd sem (without evaluations)',
    'Application mode',
    'Unemployment rate',
    'Course',
    'International',
    'Father\'s qualification',
    'Mother\'s occupation',
    'Father\'s occupation',
    'Application order',
    'Curricular units 1st sem (evaluations)',
    'Curricular units 1st sem (credited)',
    'Curricular units 2nd sem (credited)',
    'Curricular units 2nd sem (evaluations)',
    'Curricular units 1st sem (enrolled)',
    'Curricular units 2nd sem (enrolled)',
    'Curricular units 1st sem (approved)',
    'Curricular units 2nd sem (approved)',
    'Daytime/evening attendance'
]

# Drop the non-beneficial columns from the dataframe
df.drop(columns=non_beneficial_columns, inplace=True)

print(df.info())

# Updating 'Target' column to change 'Graduate' and 'enrolled' to 'still enrolled'
df['Target'] = df['Target'].replace(['Graduate', 'Enrolled'], 'Still enrolled')

# Display the updated DataFrame
print(df.head(100))

# Encoding 'df' for models

# Verify the current state of df
print("Current state of df:")
print(df.head())

# Check unique values in the Target column
print("Unique values in the Target column before encoding:", df['Target'].unique())

# Custom encoding for Target column: Dropout = 0, Enrolled and Graduate = 1
df['Target'] = df['Target'].apply(lambda x: 0 if x == 'Dropout' else 1)

# Check unique values in the Target column after encoding
print("Unique values in the Target column after encoding:", df['Target'].unique())

# Feature Scaling for numerical columns
numeric_columns = ['Age at enrollment', 'Curricular units 1st sem (grade)', 'Curricular units 2nd sem (grade)', 'GDP']
scaler = StandardScaler()
df[numeric_columns] = scaler.fit_transform(df[numeric_columns])

# Display basic information about the dataframe to confirm changes
print(df.info())

# Display the first few rows to verify the transformations
print(df.head())

# Statistical summary for numerical features
numerical_summary = df.describe()
print("Numerical Summary:\n", numerical_summary)

# Histograms for numerical features
numerical_columns = ['Age at enrollment', 'Curricular units 1st sem (grade)', 'Curricular units 2nd sem (grade)', 'GDP']

plt.figure(figsize=(12, 10))
for i, col in enumerate(numerical_columns):
    plt.subplot(2, 2, i + 1)
    sns.histplot(df[col], kde=True)
    plt.title(f'Histogram of {col}')
plt.tight_layout()
plt.show()

# Box plots for numerical features
plt.figure(figsize=(12, 10))
for i, col in enumerate(numerical_columns):
    plt.subplot(2, 2, i + 1)
    sns.boxplot(x=df[col])
    plt.title(f'Box Plot of {col}')
plt.tight_layout()
plt.show()

# Percentage distribution for categorical variables
# I firstly identify potential categorical columns
potential_categoricals = [col for col in df.columns if df[col].nunique() < 10 and col != 'Target']

# Display identified categorical columns
print("Potential Categorical Columns:", potential_categoricals)

for column in potential_categoricals:
    print(f"\nDistribution for {column}:")
    print(df[column].value_counts())
    print(df[column].value_counts(normalize=True) * 100)

# Bar plots for categorical features against the Target

categorical_columns = potential_categoricals  # Assuming these are the same as potential_categoricals

# Bar plots for categorical features against the Target
plt.figure(figsize=(15, 12))
for i, col in enumerate(categorical_columns):
    plt.subplot(3, 2, i + 1)
    sns.countplot(x=col, hue='Target', data=df)
    plt.title(f'{col} vs Target')
plt.tight_layout()
plt.show()

# Detailed statistics for continuous variables
continuous_columns = [ 'GDP']  # Example continuous variables
continuous_stats = {column: {
    'Mean': df[column].mean(),
    'Median': df[column].median(),
    'Mode': df[column].mode()[0],  # Mode can have multiple values; we take the first one
    'Standard Deviation': df[column].std(),
    'Variance': df[column].var(),
    'Range': (df[column].min(), df[column].max()),
    'Skewness': stats.skew(df[column]),
    'Kurtosis': stats.kurtosis(df[column])
} for column in continuous_columns}
print("Continuous Variables Statistics:\n", continuous_stats)

# Correlation matrix
correlation_matrix = df.corr()
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

#Examine Data Distribution to begin our check on bias and imbalance

# Display the distribution of the target variable
target_distribution = df['Target'].value_counts(normalize=True)
print("Distribution of the Target Variable:\n", target_distribution)

# Plot the distribution of the target variablE
plt.figure(figsize=(8, 6))
sns.countplot(x='Target', data=df)
plt.title('Distribution of Target Variable')
plt.show()

# Display the distribution of the gender attribute
gender_distribution = df['Gender'].value_counts(normalize=True)
print("Gender Distribution in the Dataset:\n", gender_distribution)

# Plot the distribution of the gender attribute
plt.figure(figsize=(8, 6))
sns.countplot(x='Gender', data=df)
plt.title('Distribution of Gender')
plt.show()

# Display the distribution of the age attribute
age_distribution = df['Age at enrollment'].describe()
print("Age Distribution in the Dataset:\n", age_distribution)

# Plot the distribution of the age attribute
plt.figure(figsize=(8, 6))
sns.histplot(df['Age at enrollment'], bins=20, kde=True)
plt.title('Distribution of Age at Enrollment')
plt.show()

# Distribution of target variable by gender
target_by_gender = pd.crosstab(df['Gender'], df['Target'], normalize='index')
print("\nTarget Distribution by Gender:\n", target_by_gender)

# Plot the distribution of the target variable by gender
plt.figure(figsize=(8, 6))
sns.countplot(x='Target', hue='Gender', data=df)
plt.title('Target Distribution by Gender')
plt.show()

# Create age groups with better coverage
df2 = df.copy()

df2['Age Group'] = pd.cut(df['Age at enrollment'], bins=[-1, 0, 1, 2, 3, 4, 5, 6, 7], labels=['<18', '18-25', '26-35', '36-45', '46-55', '56-65', '65-75', '>75'])

# Check the unique values in the 'Age Group' column again
print(df2['Age Group'].unique())

# Display the distribution of the 'Age Group' column again
print(df2['Age Group'].value_counts())

# Distribution of target variable by age group
target_by_age_group = pd.crosstab(df2['Age Group'], df2['Target'], normalize='index')
print("\nTarget Distribution by Age Group:\n", target_by_age_group)

# Plot the distribution of the target variable by age group
plt.figure(figsize=(8, 6))
sns.countplot(x='Target', hue='Age Group', data=df2)
plt.title('Target Distribution by Age Group')
plt.show()

# MODEL IMPLEMENTATION

# Stacked Model

X = df.drop('Target', axis=1)
y = df['Target']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Training set shape:", X_train.shape, y_train.shape)
print("Testing set shape:", X_test.shape, y_test.shape)


# Initialize the Stacking Classifier with default parameters
stacked_model_untuned = StackingClassifier(
    estimators=[
        ('lr', LogisticRegression(max_iter=10000, random_state=42)),
        ('svc', SVC(probability=True, random_state=42)),
        ('rf', RandomForestClassifier(random_state=42))
    ],
    final_estimator=LogisticRegression(max_iter=10000, random_state=42),
    cv=3
)

# Train the model
stacked_model_untuned.fit(X_train, y_train)

# Predict on the test set
y_pred_stacked_untuned = stacked_model_untuned.predict(X_test)

# Evaluate the model
print("Ensemble Model (Untuned Version)")
print("Accuracy:", accuracy_score(y_test, y_pred_stacked_untuned))
print("Classification Report:\n", classification_report(y_test, y_pred_stacked_untuned))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_stacked_untuned))



#Sample a Subset of the Data

# Sample 10% of the data for hyperparameter tuning
df_sample = df.sample(frac=0.1, random_state=42)

# Split the sampled data into features and target
X_sample = df_sample.drop('Target', axis=1)
y_sample = df_sample['Target']

# Split the dataset into training and testing sets
X_sample_train, X_sample_test, y_sample_train, y_sample_test = train_test_split(X_sample, y_sample, test_size=0.2, random_state=42)

# Define the reduced parameter grid
param_grid = {
    'lr__C': [0.1, 1],
    'lr__penalty': ['l2'],
    'svc__C': [1, 10],
    'svc__kernel': ['linear'],
    'rf__n_estimators': [50],
    'rf__max_depth': [10, 20],
    'final_estimator__C': [0.1, 1],
    'final_estimator__penalty': ['l2']
}

# Initialize the StackingClassifier with fewer base learners
stacked_model = StackingClassifier(
    estimators=[
        ('lr', LogisticRegression(max_iter=10000)),
        ('svc', SVC(probability=True)),
        ('rf', RandomForestClassifier())
    ],
    final_estimator=LogisticRegression(max_iter=10000),
    cv=3
)

# Initialize GridSearchCV with 3-fold cross-validation
grid_search = GridSearchCV(estimator=stacked_model, param_grid=param_grid, cv=3, scoring='f1', n_jobs=-1, verbose=2)

# Fit the grid search to the sampled data
grid_search.fit(X_sample_train, y_sample_train)

# Best parameters and score from the sampled data
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print("Best Parameters from Sampled Data:", best_params)
print("Best Cross-Validation Score from Sampled Data:", best_score)

# Split the full dataset into features and target
X_full = df.drop('Target', axis=1)
y_full = df['Target']

# Split the full dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2, random_state=42)

# Initialize the best model with found parameters
best_stacked_model = StackingClassifier(
    estimators=[
        ('lr', LogisticRegression(max_iter=10000, C=best_params['lr__C'], penalty=best_params['lr__penalty'])),
        ('svc', SVC(probability=True, C=best_params['svc__C'], kernel=best_params['svc__kernel'])),
        ('rf', RandomForestClassifier(n_estimators=best_params['rf__n_estimators'], max_depth=best_params['rf__max_depth']))
    ],
    final_estimator=LogisticRegression(max_iter=10000, C=best_params['final_estimator__C'], penalty=best_params['final_estimator__penalty']),
    cv=3
)

# Train the best model on the full training set
best_stacked_model.fit(X_train, y_train)
y_pred_best = best_stacked_model.predict(X_test)

# Evaluate the best model on the test set
accuracy_best = accuracy_score(y_test, y_pred_best)
precision_best = precision_score(y_test, y_pred_best)
recall_best = recall_score(y_test, y_pred_best)
f1_best = f1_score(y_test, y_pred_best)
conf_matrix_best = confusion_matrix(y_test, y_pred_best)
class_report_best = classification_report(y_test, y_pred_best)

print("\nTest Set Performance for Tuned Stacked Model:")
print(f"Accuracy: {accuracy_best:.4f}")
print(f"Precision: {precision_best:.4f}")
print(f"Recall: {recall_best:.4f}")
print(f"F1-score: {f1_best:.4f}")
print("Confusion Matrix:\n", conf_matrix_best)
print("Classification Report:\n", class_report_best)

# SVM

from sklearn.svm import SVC

# Initialize the SVM model with default parameters
svm_model_untuned = SVC(probability=True, random_state=42)

# Train the model
svm_model_untuned.fit(X_train, y_train)

# Predict on the test set
y_pred_svm_untuned = svm_model_untuned.predict(X_test)

# Evaluate the model
print("Support Vector Machine (Untuned Version)")
print("Accuracy:", accuracy_score(y_test, y_pred_svm_untuned))
print("Classification Report:\n", classification_report(y_test, y_pred_svm_untuned))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_svm_untuned))



# Split the data into features and target
X = df.drop('Target', axis=1)
y = df['Target']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the parameter grid for SVM
param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale', 'auto']
}

# Initialize GridSearchCV with SVM
grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='f1', n_jobs=-1, verbose=2)

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Best parameters and score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print("Best Parameters:", best_params)
print("Best Cross-Validation Score:", best_score)

# Initialize the SVM model with the best parameters
best_svm_model = SVC(C=best_params['C'], kernel=best_params['kernel'], gamma=best_params['gamma'], probability=True)

# Train the SVM model on the full training set
best_svm_model.fit(X_train, y_train)
y_pred_best = best_svm_model.predict(X_test)

# Evaluate the SVM model on the test set
accuracy_best = accuracy_score(y_test, y_pred_best)
precision_best = precision_score(y_test, y_pred_best)
recall_best = recall_score(y_test, y_pred_best)
f1_best = f1_score(y_test, y_pred_best)
conf_matrix_best = confusion_matrix(y_test, y_pred_best)
class_report_best = classification_report(y_test, y_pred_best)

print("\nTest Set Performance for Tuned SVM:")
print(f"Accuracy: {accuracy_best:.4f}")
print(f"Precision: {precision_best:.4f}")
print(f"Recall: {recall_best:.4f}")
print(f"F1-score: {f1_best:.4f}")
print("Confusion Matrix:\n", conf_matrix_best)
print("Classification Report:\n", class_report_best)

# Random Forest model

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Initialize the Random Forest model with default parameters
rf_model_untuned = RandomForestClassifier(random_state=42)

# Train the model
rf_model_untuned.fit(X_train, y_train)

# Predict on the test set
y_pred_rf_untuned = rf_model_untuned.predict(X_test)

# Evaluate the model
print("Random Forest (Untuned Version)")
print("Accuracy:", accuracy_score(y_test, y_pred_rf_untuned))
print("Classification Report:\n", classification_report(y_test, y_pred_rf_untuned))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_rf_untuned))



#Define Hyperparameter Grid

# Split the data into features and target
X = df.drop('Target', axis=1)
y = df['Target']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the parameter grid for Random Forest
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

# Initialize GridSearchCV with Random Forest
grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='f1', n_jobs=-1, verbose=2)

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Best parameters and score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print("Best Parameters:", best_params)
print("Best Cross-Validation Score:", best_score)

# Train and Evaluate on Full Dataset

# Initialize the Random Forest model with the best parameters
best_rf_model = RandomForestClassifier(
    n_estimators=best_params['n_estimators'],
    max_depth=best_params['max_depth'],
    min_samples_split=best_params['min_samples_split'],
    min_samples_leaf=best_params['min_samples_leaf'],
    bootstrap=best_params['bootstrap']
)

# Train the Random Forest model on the full training set
best_rf_model.fit(X_train, y_train)
y_pred_best = best_rf_model.predict(X_test)

# Evaluate the Random Forest model on the test set
accuracy_best = accuracy_score(y_test, y_pred_best)
precision_best = precision_score(y_test, y_pred_best)
recall_best = recall_score(y_test, y_pred_best)
f1_best = f1_score(y_test, y_pred_best)
conf_matrix_best = confusion_matrix(y_test, y_pred_best)
class_report_best = classification_report(y_test, y_pred_best)

print("\nTest Set Performance for Tuned Random Forest:")
print(f"Accuracy: {accuracy_best:.4f}")
print(f"Precision: {precision_best:.4f}")
print(f"Recall: {recall_best:.4f}")
print(f"F1-score: {f1_best:.4f}")
print("Confusion Matrix:\n", conf_matrix_best)
print("Classification Report:\n", class_report_best)

# Fairness and Bias Analysis for SVM and Random Forest Because SVM model is the best overall performer based on the highest F1-score and recall while Random Forest model is slightly better in terms of precision and overall accuracy.

# Function to evaluate model performance by group
def evaluate_by_group(model, X_test, y_test, group_col):
    results = {}
    for group in X_test[group_col].unique():
        X_group = X_test[X_test[group_col] == group]
        y_group = y_test[X_test[group_col] == group]
        y_pred_group = model.predict(X_group)
        report = classification_report(y_group, y_pred_group, output_dict=True)
        results[group] = report
    return results

# Evaluate Model Performance by Gender

# Assume X_test and y_test are defined as the test sets after resampling and preprocessing
gender_results_svm = evaluate_by_group(best_svm_model, X_test, y_test, 'Gender')
print("\nSVM Model Performance by Gender:\n", gender_results_svm)

# Evaluate Random Forest model by gender
gender_results_rf = evaluate_by_group(best_rf_model, X_test, y_test, 'Gender')
print("\nRandom Forest Model Performance by Gender:\n", gender_results_rf)

# Evaluate Model Performance by GDP

# Evaluate SVM model by GDP
GDP_results_svm = evaluate_by_group(best_svm_model, X_test, y_test, 'GDP')
print("\nSVM Model Performance by GDP:\n", GDP_results_svm)

# Evaluate Random Forest model by GDP
GDP_rf = evaluate_by_group(best_rf_model, X_test, y_test, 'GDP')
print("\nRandom Forest Model Performance by GDP:\n", GDP_rf)

# Evaluate Model Performance by Debtor

# Evaluate SVM model by Debtor
Debtor_svm = evaluate_by_group(best_svm_model, X_test, y_test, 'Debtor')
print("\nSVM Model Performance by Debtor:\n", Debtor_svm)

# Evaluate Random Forest model by Debtor
Debtor_rf = evaluate_by_group(best_rf_model, X_test, y_test, 'Debtor')
print("\nRandom Forest Model Performance by Debtor:\n", Debtor_rf)

# Evaluate Model Performance by Scholarship holder

# Evaluate SVM model by Scholarship holder
Scholarship_holder_svm = evaluate_by_group(best_svm_model, X_test, y_test, 'Scholarship holder')
print("\nSVM Model Performance by Scholarship holder:\n", Scholarship_holder_svm)

# Evaluate Random Forest model by Scholarship holder
Scholarship_holder_rf = evaluate_by_group(best_rf_model, X_test, y_test, 'Scholarship holder')
print("\nRandom Forest Model Performance by Scholarship holder:\n", Scholarship_holder_rf)

# Results from the tuned models
results = {
    'Model': ['Random Forest', 'SVM', 'Ensemble'],
    'Accuracy': [0.8316, 0.8282, 0.8380],
    'Precision': [0.8292, 0.8126, 0.8174]
}

# Convert to DataFrame
results_df = pd.DataFrame(results)
print(results_df)

import matplotlib.pyplot as plt
import seaborn as sns

# Set plot style
sns.set(style="whitegrid")

# Combined Bar Plot for Accuracy and Precision
fig, ax = plt.subplots(figsize=(12, 6))
bar_width = 0.35
index = range(len(results_df))

# Plot Accuracy bars
bars1 = plt.bar(index, results_df['Accuracy'], bar_width, label='Accuracy', color='b')

# Plot Precision bars
bars2 = plt.bar([i + bar_width for i in index], results_df['Precision'], bar_width, label='Precision', color='orange')

# Add labels, title, and legend
plt.xlabel('Model')
plt.ylabel('Score')
plt.title('Accuracy and Precision of Models')
plt.xticks([i + bar_width / 2 for i in index], results_df['Model'])
plt.ylim(0.8, 0.85)
plt.legend()

# Display the plot
plt.tight_layout()
plt.show()
